# Information Retrieval Lab: Fake News Classification
This repository contains code implemented as a part of the Information Retrieval Lab at the University of Bonn in 2021. The project was created along with two other contibutors. All files (code,images etc.) in this repo, except the project report, are my contributions to the project.

### Contents of the repo:
<ul>
<li>Notebook code files</li>
<li>Code for the app</li>
<li>Presentation slides with an overview of results obtained</li> 
<li>Report with explanations and results for models from all group members</li> 
</ul>


**Abstract:** The spread of fake news on the internet is a matter of serious concern due to its potential to cause social and national damage. If left unchecked, it can have disastrous consequences like nationwide mistrust in public institutions. A lot of existing research is focused on the challenging task of automatic fake news detection. This project analyzes some of the research in the domain and explores various machine learning and deep learning models. We present a comparison of these models using natural language processing tools to analyze news articles and classify them based on their textual characteristics. Classification performance is evaluated using different metrics comparing pre-processing and word embedding techniques. We show how explainable artificial intelligence can be used to understand the predictions of
these black-box models which can help users decide if they should trust the model. Lastly, we discuss our findings and show two implementations of our models as web applications.  <br>
**Keywords**: Fake news · Fact-checking · Deep Learning · Explainable AI
<br>

### Tasks:
<ul>
<li>Analyze: How well can automated methods perform on the given task?</li>
<li>Compared performance of ML and Deep Learning models</li>
<li>Used and analyzed performance of various <b>feature engineering</b> methods for prediction as well as to extract statistical insights  <br> 
  <ul>
  <li> Implemented and compared performance with methods from <a href="https://arxiv.org/abs/1703.09398">reference paper 1</a>
  <li> Analyzed reproducibility based on <a href="https://link.springer.com/chapter/10.1007/978-3-030-72240-1_9">reference paper 2</a>
  </ul>
  </li>
<li>Used <b>explainability methods</b> to make our model predictions explainable.</li>
</ul>

### **UI of the app:**
***
![alt text](https://github.com/rohilrao/IR-NLP-Fake-News/blob/main/src/images/UI_Exp.PNG)
***

